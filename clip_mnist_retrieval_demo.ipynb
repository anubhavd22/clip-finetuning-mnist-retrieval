{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fd860b0",
   "metadata": {},
   "source": [
    "# 1) In this project, we will learn to use the Meta AI's CLIP model for image retrieval task. \n",
    "# 2) We will also learn how to fine tune the CLIP model on the MNIST dataset to improve it's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148f8d6",
   "metadata": {},
   "source": [
    "### 1. Importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa6e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core ML & Data ---\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# --- Plotting & Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "\n",
    "# --- App & Utilities ---\n",
    "import gradio as gr\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ec28a",
   "metadata": {},
   "source": [
    "### 2. Load Base CLIP Model & Print Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe1e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading original pre-trained CLIP model (ViT-B/32)...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the model onto the correct device\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "# torch.save(model, 'clip_vit_b32.pth') # For saving the model locally.\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# --- Print Model Details ---\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(f\"Model parameters:   {np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(f\"Input resolution:   {input_resolution}x{input_resolution}\")\n",
    "print(f\"Context length:     {context_length} tokens\")\n",
    "print(f\"Vocabulary size:    {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f49603",
   "metadata": {},
   "source": [
    "### 3. Loading the MNIST dataset and creating a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c96a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 10,000-image TEST set (train=False is correct for testing)\n",
    "# 'transform=preprocess' automatically applies CLIP's preprocessing to each image\n",
    "mnist_data = datasets.MNIST(root='./data', train=False, download=True, transform=preprocess)\n",
    "\n",
    "# Set up the DataLoader to process the images in efficient batches\n",
    "batch_size = 128\n",
    "mnist_loader = DataLoader(mnist_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af7016",
   "metadata": {},
   "source": [
    "### 4. About the MNIST dataset (Sample Visualizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the original PIL images for plotting\n",
    "original_mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(\"These are grayscale images of handwritten digits (0-9).\")\n",
    "print(f\"Number of samples in MNIST test set: {len(original_mnist_test)}\")\n",
    "# Get the first image to show its type (it's a PIL Image, not a tensor)\n",
    "image, label = original_mnist_test[0]\n",
    "print(f\"Original image size: {image.size}\")\n",
    "\n",
    "# Show some sample images from the *original* dataset\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "fig.suptitle('Sample Images from MNIST Test Set', fontsize=16) # Add a main title\n",
    "\n",
    "for i in range(5):\n",
    "    # Get the original PIL image\n",
    "    image, label = original_mnist_test[i]\n",
    "    \n",
    "    # imshow can plot PIL images directly\n",
    "    axes[i].imshow(image, cmap='gray') \n",
    "    axes[i].set_title(f\"Label: {label}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234afae",
   "metadata": {},
   "source": [
    "### 5. Calculate CLIP image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting embedding generation for 10,000 test images...\")\n",
    "\n",
    "all_embeddings_list = []\n",
    "all_labels_list = []\n",
    "\n",
    "# Use torch.no_grad() to save memory and compute (disables gradient calculation)\n",
    "with torch.no_grad():\n",
    "    # Loop through the DataLoader, which yields preprocessed batches\n",
    "    for images, labels in tqdm(mnist_loader):\n",
    "        # Move the image batch to the GPU\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Calculate embeddings for the entire batch at once\n",
    "        batch_embeddings = model.encode_image(images)\n",
    "        \n",
    "        # Move embeddings and labels back to the CPU for storage\n",
    "        all_embeddings_list.append(batch_embeddings.cpu())\n",
    "        all_labels_list.append(labels.cpu())\n",
    "\n",
    "print(\"Embedding generation complete.\")\n",
    "\n",
    "# --- Concatenate and Save ---\n",
    "print(\"Concatenating all batches...\")\n",
    "\n",
    "# 'torch.cat' stacks the list of batch tensors into one big tensor\n",
    "final_embeddings = torch.cat(all_embeddings_list)\n",
    "final_labels = torch.cat(all_labels_list)\n",
    "\n",
    "print(f\"Final embeddings shape: {final_embeddings.shape}\")\n",
    "print(f\"Final labels shape: {final_labels.shape}\")\n",
    "\n",
    "# Save embeddings and labels for future use in the app\n",
    "# (Make sure the 'checkpoints' directory exists!)\n",
    "torch.save(final_embeddings, 'checkpoints/mnist_clip_image_embeddings_test.pt')\n",
    "torch.save(final_labels, 'checkpoints/mnist_labels_test.pt')\n",
    "\n",
    "print(\"Embeddings and labels saved to 'checkpoints/'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0034cc6",
   "metadata": {},
   "source": [
    "# TESTING TIME....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ccaac",
   "metadata": {},
   "source": [
    "### 6. Loading embeddings and reducing dimensionality using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0dd03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading pre-computed test set embeddings...\")\n",
    "# 1) Load saved embeddings and labels (created from the test set)\n",
    "loaded_embeddings = torch.load('checkpoints/mnist_clip_image_embeddings_test.pt', map_location=device)\n",
    "loaded_labels = torch.load('checkpoints/mnist_labels_test.pt', map_location=device)\n",
    "\n",
    "# 2) Load original MNIST *TEST* dataset (for displaying results)\n",
    "# --- THIS IS THE CRITICAL FIX ---\n",
    "original_mnist = datasets.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "# --- 3) Prepare data for UMAP (on CPU) ---\n",
    "print(\"Pre-computing UMAP projection for 10,000 test images...\")\n",
    "embeddings_np = loaded_embeddings.cpu().numpy()\n",
    "labels_np = loaded_labels.cpu().numpy()\n",
    "\n",
    "umap_embeddings_np = embeddings_np\n",
    "umap_labels_np = labels_np\n",
    "\n",
    "# --- 4) Fit UMAP Reducer ---\n",
    "# This 'reducer' object will be re-used to transform new queries\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    n_components=2,\n",
    "    metric='cosine',\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "print(\"Fitting UMAP reducer...\")\n",
    "umap_embeddings_2d = reducer.fit_transform(umap_embeddings_np)\n",
    "print(\"UMAP pre-computation complete.\")\n",
    "\n",
    "# --- 5) Prepare Embeddings for Search (on GPU) ---\n",
    "# Create a copy on the GPU for fast similarity calculations in the app\n",
    "all_features_gpu = loaded_embeddings.to(device)\n",
    "\n",
    "print(\"App setup is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec70305",
   "metadata": {},
   "source": [
    "### 7. Defining a function to calculate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9135692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate cosine similarity\n",
    "def calculate_similarity(feature1, feature2):\n",
    "    # Normalize the first set of features (unit vectors)\n",
    "    feature1_norm = feature1 / feature1.norm(dim=-1, keepdim=True)\n",
    "    # Normalize the second set of features (unit vectors)\n",
    "    feature2_norm = feature2 / feature2.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate the dot product between all vectors\n",
    "    # For unit vectors, dot product == cosine similarity\n",
    "    similarity = (feature1_norm @ feature2_norm.T) \n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16afc2b2",
   "metadata": {},
   "source": [
    "### 8. Function for Text-to-Image Search üîé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee99049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search_and_plot(digit_text, top_k=3):\n",
    "    if not digit_text:\n",
    "        print(\"Please provide a digit text (0-9).\")\n",
    "        return [], None\n",
    "    \n",
    "    # 1. Create a mapping from digit character to word\n",
    "    digit_map = {\n",
    "        \"0\": \"zero\", \"1\": \"one\", \"2\": \"two\", \"3\": \"three\", \"4\": \"four\",\n",
    "        \"5\": \"five\", \"6\": \"six\", \"7\": \"seven\", \"8\": \"eight\", \"9\": \"nine\"\n",
    "    }\n",
    "    \n",
    "    # 2. Get the word for the corresponding digit\n",
    "    digit_word = digit_map.get(digit_text, \"\") # Get the word, or empty string if not found\n",
    "\n",
    "    if not digit_word: # Check if the map lookup failed\n",
    "        print(f\"Invalid digit text: {digit_text}\")\n",
    "        return [], None\n",
    "    \n",
    "    print(f\"Searching for text: A handwritten digit {digit_word}\")\n",
    "\n",
    "    # 3. Encoding the text prompt using the word\n",
    "    prompt = f\"A handwritten digit {digit_word}\"\n",
    "    text_token = clip.tokenize([prompt]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_token)\n",
    "\n",
    "    # 4. Calculating similarity\n",
    "    similarities = calculate_similarity(text_features, all_features_gpu)\n",
    "\n",
    "    # 5. Retrieving top-k similar images\n",
    "    _, top_indices = torch.topk(similarities.squeeze(0), top_k)\n",
    "    top_indices = top_indices.cpu().numpy().tolist()\n",
    "    print(f\"Top {top_k} similar images indices: {top_indices}\")\n",
    "    \n",
    "    retrieved_labels = [labels_np[idx].item() for idx in top_indices]\n",
    "    print(f\"Labels for top indices: {retrieved_labels}\")\n",
    "\n",
    "    # 6. Retrieving original images\n",
    "    result_images = []\n",
    "    for idx in top_indices:\n",
    "        image, _ = original_mnist[idx] # Uses 'original_mnist' (test set)\n",
    "        result_images.append(image)\n",
    "\n",
    "    # --- 7. Generate Plot ---\n",
    "    text_features_cpu = text_features.cpu().numpy()\n",
    "    text_embedding_2d = reducer.transform(text_features_cpu) # Uses 'reducer'\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=umap_embeddings_2d[:, 0], # Uses 'umap_embeddings_2d'\n",
    "        y=umap_embeddings_2d[:, 1],\n",
    "        hue=umap_labels_np,        # Uses 'umap_labels_np'\n",
    "        palette=sns.color_palette(\"tab10\", 10),\n",
    "        s=10, alpha=0.6,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # Plot the text embedding point\n",
    "    ax.scatter(\n",
    "        text_embedding_2d[0, 0], text_embedding_2d[0, 1],\n",
    "        marker='d',         \n",
    "        color='black',\n",
    "        s=100,                   \n",
    "        label=f'Text: \"{prompt}\"'\n",
    "    )\n",
    "\n",
    "    ax.set_title('UMAP Projection (Image Embeddings + Text Query)')\n",
    "    ax.set_xlabel('UMAP Component 1')\n",
    "    ax.set_ylabel('UMAP Component 2')\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Legend marker size fix\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    dot_marker_size_scale = 6\n",
    "    for i in range(min(10, len(handles) -1)):\n",
    "         if hasattr(handles[i], 'set_sizes'):\n",
    "             current_size = handles[i].get_sizes()[0]\n",
    "             handles[i].set_sizes([current_size * dot_marker_size_scale])\n",
    "    ax.legend(handles, labels, title='Digit Label / Text', loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    plt.close(fig)\n",
    "\n",
    "    return result_images, fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c46d5",
   "metadata": {},
   "source": [
    "### 9. Function for Sketch-to-Image Search ‚úèÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6529d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function for Sketch Preprocessing ---\n",
    "def preprocess_sketch(sketch_input_data):\n",
    "    \"\"\"\n",
    "    Processes a user sketch (from Gradio) to resemble an MNIST digit.\n",
    "    Args:\n",
    "        sketch_input_data (np.ndarray or dict): The sketch data from Gradio.\n",
    "    Returns:\n",
    "        PIL.Image or None: A preprocessed PIL Image, ready for CLIP's preprocess, or None if invalid.\n",
    "    \"\"\"\n",
    "    sketch_array = None # Initialize sketch_array\n",
    "\n",
    "    if sketch_input_data is None:\n",
    "        print(\"Sketch input is None.\")\n",
    "        return None\n",
    "\n",
    "    # Check if input is a dictionary and extract the numpy array\n",
    "    if isinstance(sketch_input_data, dict):\n",
    "        print(\"Sketch input is a dict. Keys:\", sketch_input_data.keys()) # DEBUG: See keys\n",
    "        if 'image' in sketch_input_data and isinstance(sketch_input_data['image'], np.ndarray):\n",
    "             sketch_array = sketch_input_data['image']\n",
    "        elif 'composite' in sketch_input_data and isinstance(sketch_input_data['composite'], np.ndarray):\n",
    "             sketch_array = sketch_input_data['composite']\n",
    "        else:\n",
    "             print(\"Could not find a valid NumPy array in the sketch dictionary.\")\n",
    "             return None # Cannot proceed\n",
    "    elif isinstance(sketch_input_data, np.ndarray):\n",
    "        sketch_array = sketch_input_data # Input is already the array\n",
    "    else:\n",
    "        print(f\"Unexpected sketch input type: {type(sketch_input_data)}\")\n",
    "        return None\n",
    "\n",
    "    if sketch_array is None:\n",
    "         print(\"Failed to extract sketch array.\")\n",
    "         return None\n",
    "    if sketch_array.size == 0:\n",
    "         print(\"Received empty sketch array.\")\n",
    "         return None\n",
    "\n",
    "    try:\n",
    "        pil_image = Image.fromarray(sketch_array).convert(\"L\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Image.fromarray: {e}\")\n",
    "        print(f\"Sketch array shape: {sketch_array.shape}, dtype: {sketch_array.dtype}\")\n",
    "        return None\n",
    "\n",
    "    pil_image = ImageOps.invert(pil_image)\n",
    "\n",
    "    bbox = pil_image.getbbox()\n",
    "    if bbox is None:\n",
    "        print(\"No content found in the sketch (bounding box is None).\")\n",
    "        return Image.new('L', (28, 28), color=0)\n",
    "\n",
    "    pil_image = pil_image.crop(bbox)\n",
    "\n",
    "    width, height = pil_image.size\n",
    "    target_size = max(width, height)\n",
    "    padding_left = (target_size - width) // 2\n",
    "    padding_top = (target_size - height) // 2\n",
    "    padding_right = target_size - width - padding_left\n",
    "    padding_bottom = target_size - height - padding_top\n",
    "    padding = (padding_left, padding_top, padding_right, padding_bottom)\n",
    "    pil_image = ImageOps.expand(pil_image, padding, fill=0)\n",
    "\n",
    "    pil_image = pil_image.resize((28, 28), Image.Resampling.LANCZOS)\n",
    "\n",
    "    return pil_image\n",
    "\n",
    "# --- Function for Sketch Search AND Plotting ---\n",
    "def sketch_search_and_plot(sketch_image, top_k=3):\n",
    "    \"\"\"\n",
    "    Finds top_k MNIST images similar to the user's sketch.\n",
    "    Args:\n",
    "        sketch_image (np.ndarray or dict): Sketch data from Gradio.\n",
    "        top_k (int): Number of results.\n",
    "    Returns:\n",
    "        tuple: (List of result PIL Images, Matplotlib Figure object or None)\n",
    "    \"\"\"\n",
    "    # Check 1: Input is None (handled by preprocess_sketch now, but good practice)\n",
    "    if sketch_image is None:\n",
    "        print(\"Please provide a sketch image.\")\n",
    "        return [], None\n",
    "\n",
    "    print(\"Processing sketch...\")\n",
    "    # --- 1. Preprocess the Sketch ---\n",
    "    # Use the sketch_input_data variable name consistently\n",
    "    preprocessed_sketch_pil = preprocess_sketch(sketch_image)\n",
    "\n",
    "    # Check 2: Preprocessing failed\n",
    "    if preprocessed_sketch_pil is None:\n",
    "        print(\"Sketch preprocessing failed.\")\n",
    "        return [], None\n",
    "\n",
    "    # --- 2. Encode the Processed Sketch using CLIP ---\n",
    "    # Apply CLIP's standard preprocessing AFTER our custom preprocessing\n",
    "    # Ensure 'preprocess' and 'device' are available in this scope\n",
    "    clip_input_tensor = preprocess(preprocessed_sketch_pil).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        # Ensure 'model' is available in this scope\n",
    "        sketch_features = model.encode_image(clip_input_tensor)\n",
    "\n",
    "    # --- 3. Search (using all_features_gpu) ---\n",
    "    print(\"Calculating similarities...\")\n",
    "    # Ensure 'calculate_similarity' and 'all_features_gpu' are available\n",
    "    similarities = calculate_similarity(sketch_features, all_features_gpu)\n",
    "    _, top_indices = torch.topk(similarities.squeeze(0), top_k)\n",
    "    top_indices = top_indices.cpu().numpy().tolist()\n",
    "    print(f\"Top sketch indices: {top_indices}\")\n",
    "\n",
    "    # Ensure 'original_mnist' is available\n",
    "    result_images = [original_mnist[idx][0] for idx in top_indices]\n",
    "\n",
    "    # --- 4. Generate Plot ---\n",
    "    try:\n",
    "        sketch_features_cpu = sketch_features.cpu().numpy()\n",
    "        # Ensure 'reducer' is available (the fitted UMAP object)\n",
    "        sketch_embedding_2d = reducer.transform(sketch_features_cpu)\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "        # Ensure 'umap_embeddings_2d' and 'umap_labels_np' are available\n",
    "        sns.scatterplot(\n",
    "            x=umap_embeddings_2d[:, 0], y=umap_embeddings_2d[:, 1],\n",
    "            hue=umap_labels_np, palette=sns.color_palette(\"tab10\", 10),\n",
    "            s=10, alpha=0.6, ax=ax\n",
    "        )\n",
    "        ax.scatter(\n",
    "            sketch_embedding_2d[0, 0], sketch_embedding_2d[0, 1],\n",
    "            marker='d', color='black', s=100, label='Your Sketch'\n",
    "        )\n",
    "        ax.set_title('UMAP Projection (Image Embeddings + Sketch Query)')\n",
    "        ax.set_xlabel('UMAP Component 1')\n",
    "        ax.set_ylabel('UMAP Component 2')\n",
    "        ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        dot_marker_size_scale = 6\n",
    "        for i in range(min(10, len(handles) -1)):\n",
    "             if hasattr(handles[i], 'set_sizes'):\n",
    "                 current_size = handles[i].get_sizes()[0]\n",
    "                 handles[i].set_sizes([current_size * dot_marker_size_scale])\n",
    "        ax.legend(handles, labels, title='Digit Label / Sketch', loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "        plt.close(fig)\n",
    "        plot_output = fig\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating plot for sketch: {e}\")\n",
    "        plot_output = None # Return None if plotting fails\n",
    "\n",
    "    return result_images, plot_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce712c6",
   "metadata": {},
   "source": [
    "## üöÄ Launching the App!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cbd209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_sketch():\n",
    "    return None\n",
    "\n",
    "# --- Gradio Interface ---\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as app_interface:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # CLIP MNIST Search Engine üß†üñºÔ∏è‚úèÔ∏è\n",
    "        Use CLIP embeddings to search the MNIST dataset using text prompts or sketches.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Tabs():\n",
    "        # --- Text Search Tab ---\n",
    "        with gr.TabItem(\"Text Search\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    text_input = gr.Radio(\n",
    "                        choices=[str(i) for i in range(10)],\n",
    "                        label=\"Select a Digit\",\n",
    "                        info=\"Choose the digit you want to search for.\"\n",
    "                    )\n",
    "                    submit_btn_text = gr.Button(\"Search Text\", variant=\"primary\")\n",
    "                with gr.Column(scale=3):\n",
    "                    gallery_text = gr.Gallery(\n",
    "                        label=\"Search Results\", columns=3, object_fit=\"contain\",\n",
    "                        height=250, preview=True\n",
    "                    )\n",
    "                    plot_text = gr.Plot(label=\"UMAP Visualization (Text Query)\")\n",
    "\n",
    "        # --- Sketch Search Tab ---\n",
    "        with gr.TabItem(\"Sketch Search\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    # Add Sketchpad input\n",
    "                    sketch_input = gr.Sketchpad(\n",
    "                        label=\"Draw a Digit (0-9)\",\n",
    "                        type=\"numpy\", # Output as numpy array\n",
    "                     )\n",
    "                    submit_btn_sketch = gr.Button(\"Search Sketch\", variant=\"primary\")\n",
    "                    clear_btn_sketch = gr.Button(\"Clear Sketch\") # Add a clear button\n",
    "                with gr.Column(scale=3):\n",
    "                     gallery_sketch = gr.Gallery(\n",
    "                        label=\"Search Results\", columns=3, object_fit=\"contain\",\n",
    "                        height=250, preview=True\n",
    "                    )\n",
    "                     plot_sketch = gr.Plot(label=\"UMAP Visualization (Sketch Query)\")\n",
    "\n",
    "\n",
    "    # --- Event Handlers ---\n",
    "    submit_btn_text.click(\n",
    "        fn=text_search_and_plot,\n",
    "        inputs=text_input,\n",
    "        outputs=[gallery_text, plot_text],\n",
    "        show_progress=\"full\"\n",
    "    )\n",
    "    submit_btn_sketch.click(\n",
    "        fn=sketch_search_and_plot,\n",
    "        inputs=sketch_input,\n",
    "        outputs=[gallery_sketch, plot_sketch],\n",
    "        show_progress=\"full\"\n",
    "    )\n",
    "\n",
    "    # Link clear button to sketchpad input\n",
    "    clear_btn_sketch.click(fn=clear_sketch, inputs=None, outputs=sketch_input)\n",
    "    \n",
    "# Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    app_interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b14a06c",
   "metadata": {},
   "source": [
    "# <span style=\"color:green;\">Fine-tuning CLIP on MNIST dataset</span> üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432670a",
   "metadata": {},
   "source": [
    "### 1. Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46300f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core ML & Data ---\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# --- Plotting & Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "\n",
    "# --- App & Utilities ---\n",
    "import gradio as gr\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de3c00e",
   "metadata": {},
   "source": [
    "### 2. Preparing custom image-text paired dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP needs (image, text) pairs for fine-tuning.\n",
    "# A custom Dataset class is created here to wrap the standard MNIST dataset, which only provides (image, label).\n",
    "# This class will generate the required text prompts on the fly.\n",
    "\n",
    "class PairedMNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset that takes an MNIST dataset and the\n",
    "    CLIP preprocessor, returning (preprocessed_image, tokenized_text) pairs.\n",
    "    \"\"\"\n",
    "    def __init__(self, mnist_dataset, clip_preprocess):\n",
    "        self.mnist = mnist_dataset\n",
    "        self.preprocess = clip_preprocess\n",
    "        \n",
    "        # Maps numeric labels (e.g., 0) to their text words (e.g., \"zero\")\n",
    "        # to create meaningful text prompts.\n",
    "        self.digit_map = {\n",
    "            0: \"zero\", 1: \"one\", 2: \"two\", 3: \"three\", 4: \"four\",\n",
    "            5: \"five\", 6: \"six\", 7: \"seven\", 8: \"eight\", 9: \"nine\"\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is the length of the original MNIST set.\n",
    "        return len(self.mnist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # This method is called by the DataLoader for each item.\n",
    "        \n",
    "        # 1. Get the original PIL image and its numeric label.\n",
    "        image, label = self.mnist[idx]\n",
    "        \n",
    "        # 2. Apply the CLIP preprocessing to the image.\n",
    "        #    The original 'image' is a PIL Image, so the preprocess function\n",
    "        #    (handling resize, 3-channel conversion, normalization) works directly.\n",
    "        preprocessed_image = self.preprocess(image)\n",
    "\n",
    "        # 3. Create the text prompt.\n",
    "        #    Maps the label (e.g., 7) to its word (\"seven\").\n",
    "        digit_word = self.digit_map[label]\n",
    "        prompt = f\"a handwritten digit {digit_word}\"\n",
    "        \n",
    "        # 4. Tokenize the text prompt.\n",
    "        #    clip.tokenize() returns a batch, so the first [0]\n",
    "        #    item is selected.\n",
    "        tokenized_text = clip.tokenize([prompt])[0]\n",
    "        \n",
    "        # Return the (image, text) pair.\n",
    "        return preprocessed_image, tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2504191f",
   "metadata": {},
   "source": [
    "### 3. Define the CLIP Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1901f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clip_loss(logits_per_image, logits_per_text, device):\n",
    "    \"\"\"\n",
    "    Calculates the symmetric CLIP loss (contrastive loss).\n",
    "    \"\"\"\n",
    "    # Create the \"ground truth\" labels. For a batch of size N,\n",
    "    # the i-th image matches the i-th text, so the labels are [0, 1, 2, ..., N-1].\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    ground_truth = torch.arange(batch_size, dtype=torch.long, device=device)\n",
    "    \n",
    "    # Calculate cross-entropy loss in both directions\n",
    "    loss_img = F.cross_entropy(logits_per_image, ground_truth)\n",
    "    loss_txt = F.cross_entropy(logits_per_text, ground_truth)\n",
    "    \n",
    "    # The final loss is the average of the two\n",
    "    total_loss = (loss_img + loss_txt) / 2\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df375bf",
   "metadata": {},
   "source": [
    "### 4. Loading model, mnist data, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf140fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20  # Set desired number of epochs\n",
    "LR = 1e-5    # Using the stabler 1e-5 learning rate. 1e-4 is very high.\n",
    "\n",
    "# --- Load Your Fine-Tuned Model ---\n",
    "# This loads the model you already trained (continuing from a checkpoint)\n",
    "print(\"Loading fine-tuned model from checkpoint...\")\n",
    "model_path = \"checkpoints/finetuned_mnist_clip.pt\"\n",
    "\n",
    "# 1. Load the original \"ViT-B/32\" architecture\n",
    "#    (jit=False is required to load a state_dict)\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "\n",
    "# 2. Load your fine-tuned weights into that architecture\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "print(f\"Successfully loaded weights from {model_path}\")\n",
    "\n",
    "# --- Load MNIST Training Data ---\n",
    "# We need the original PIL Images (transform=None) for our custom dataset\n",
    "mnist_train = MNIST(root=\".\", train=True, download=True, transform=None)\n",
    "\n",
    "# --- Create Paired Dataset and DataLoader ---\n",
    "# This uses the class from Cell 2\n",
    "paired_dataset = PairedMNISTDataset(mnist_dataset=mnist_train, clip_preprocess=preprocess)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    paired_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True  # Shuffle=True is crucial for training\n",
    ")\n",
    "\n",
    "# --- Define the Optimizer (This was missing!) ---\n",
    "print(\"Freezing logit_scale and creating SGD optimizer...\")\n",
    "# 1. Freeze the logit_scale to prevent training instability (from our debugging)\n",
    "model.logit_scale.requires_grad = False\n",
    "\n",
    "# 2. Create the stable SGD optimizer\n",
    "#    We filter to ensure the optimizer only sees trainable parameters\n",
    "optimizer = torch.optim.SGD(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=LR,\n",
    "    momentum=0.9 \n",
    ")\n",
    "\n",
    "print(\"Setup complete. Ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02810e3c",
   "metadata": {},
   "source": [
    "### 5. Running the Fine-Tuning Loop üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. FREEZE LOGIT_SCALE (Run before optimizer) ---\n",
    "print(\"Freezing logit_scale parameter...\")\n",
    "model.logit_scale.requires_grad = False\n",
    "\n",
    "# --- 2. THE FINAL FIX: Switch Optimizer to SGD ---\n",
    "# Adam is too unstable. SGD is simpler and more stable.\n",
    "LR = 1e-4 # A good, safe starting learning rate for SGD\n",
    "print(f\"Using SGD optimizer with LR={LR}\")\n",
    "optimizer = torch.optim.SGD(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), # Filtered list\n",
    "    lr=LR,\n",
    "    momentum=0.9 # Standard momentum\n",
    ")\n",
    "\n",
    "# --- 3. RUN THE TRAINING LOOP ---\n",
    "print(\"Starting fine-tuning with SGD...\")\n",
    "model.train() # Set model to training mode\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"--- Epoch {epoch + 1}/{EPOCHS} ---\")\n",
    "    \n",
    "    for (images, texts) in tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        # We'll keep forcing float32 for safety\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float32):\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "            loss = calculate_clip_loss(logits_per_image, logits_per_text, device)\n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            print(\"!!! Loss became NAN. Stopping training. !!!\")\n",
    "            break\n",
    "\n",
    "        # --- Backward Pass & Optimize ---\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping is still a good idea\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "    if 'loss' in locals() and torch.isnan(loss):\n",
    "        print(\"Epoch stopped early due to nan loss.\")\n",
    "        break\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} finished. Last batch loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced238e",
   "metadata": {},
   "source": [
    "### 5. Saving the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d07c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save the fine-tuned model's weights ---\n",
    "# We'll save it to a new file\n",
    "save_path = \"checkpoints/finetuned_mnist_clip.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Fine-tuned model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae755a2",
   "metadata": {},
   "source": [
    "### 6. Running the app with the fine-tuned model üî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa53507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup ---\n",
    "print(\"Setting up model and loading data...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "finetuned_model_path = \"checkpoints/finetuned_mnist_clip.pt\"\n",
    "\n",
    "# --- 2. Load Your Fine-Tuned Model ---\n",
    "# First, load the base model architecture (on CPU, with jit=False)\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=\"cpu\", jit=False)\n",
    "# Now, load your fine-tuned weights into that architecture\n",
    "model.load_state_dict(torch.load(finetuned_model_path, map_location=\"cpu\"))\n",
    "# Move the fully-loaded model to the GPU and set to eval mode\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"Successfully loaded fine-tuned model from {finetuned_model_path}\")\n",
    "\n",
    "# --- 3. Load MNIST Test Data ---\n",
    "# Load the test set (train=False) with the CLIP preprocessor\n",
    "# We'll use this to generate new embeddings\n",
    "mnist_test_transformed = MNIST(root=\".\", train=False, download=True, transform=preprocess)\n",
    "# Also load the original test set (for displaying results)\n",
    "original_mnist_test = MNIST(root=\".\", train=False, download=True)\n",
    "\n",
    "# --- 4. Generate NEW Embeddings from Fine-Tuned Model ---\n",
    "# We must re-create the embeddings using the *new* model\n",
    "print(\"Generating new embeddings for the 10,000 test images...\")\n",
    "batch_size = 128\n",
    "data_loader = DataLoader(mnist_test_transformed, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "all_embeddings_list = []\n",
    "all_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(data_loader):\n",
    "        images = images.to(device)\n",
    "        batch_embeddings = model.encode_image(images)\n",
    "        all_embeddings_list.append(batch_embeddings.cpu())\n",
    "        all_labels_list.append(labels.cpu())\n",
    "\n",
    "# Concatenate all batches into single tensors\n",
    "all_features_cpu = torch.cat(all_embeddings_list)\n",
    "all_labels_cpu = torch.cat(all_labels_list)\n",
    "all_features_gpu = all_features_cpu.to(device) # For fast search\n",
    "print(\"New test embeddings generated.\")\n",
    "\n",
    "# --- 5. Pre-compute UMAP (Run Once at Startup) ---\n",
    "print(\"Pre-computing UMAP projection...\")\n",
    "embeddings_np = all_features_cpu.numpy()\n",
    "labels_np = all_labels_cpu.numpy()\n",
    "umap_embeddings_np = embeddings_np # Use all 10k points\n",
    "umap_labels_np = labels_np\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=15, min_dist=0.1, n_components=2,\n",
    "    metric='cosine', random_state=42, verbose=False\n",
    ")\n",
    "print(\"Fitting UMAP reducer...\")\n",
    "umap_embeddings_2d = reducer.fit_transform(umap_embeddings_np)\n",
    "print(\"UMAP pre-computation complete.\")\n",
    "\n",
    "# --- 6. Helper & Search Functions ---\n",
    "\n",
    "def calculate_similarity(features1, features2):\n",
    "    features1 = features1 / features1.norm(dim=-1, keepdim=True)\n",
    "    features2 = features2 / features2.norm(dim=-1, keepdim=True)\n",
    "    similarity = features1 @ features2.T\n",
    "    return similarity\n",
    "\n",
    "def clear_sketch():\n",
    "    return None\n",
    "\n",
    "# --- Text Search ---\n",
    "def text_search_and_plot(digit_text, top_k=3):\n",
    "    if not digit_text:\n",
    "        return [], None\n",
    "    \n",
    "    digit_map = {\n",
    "        \"0\": \"zero\", \"1\": \"one\", \"2\": \"two\", \"3\": \"three\", \"4\": \"four\",\n",
    "        \"5\": \"five\", \"6\": \"six\", \"7\": \"seven\", \"8\": \"eight\", \"9\": \"nine\"\n",
    "    }\n",
    "    digit_word = digit_map.get(digit_text, \"\")\n",
    "    if not digit_word:\n",
    "        return [], None\n",
    "\n",
    "    print(f\"Searching for text: A handwritten digit {digit_word}\")\n",
    "    prompt = f\"A handwritten digit {digit_word}\"\n",
    "    text_token = clip.tokenize([prompt]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_token)\n",
    "\n",
    "    similarities = calculate_similarity(text_features, all_features_gpu)\n",
    "    _, top_indices = torch.topk(similarities.squeeze(0), top_k)\n",
    "    top_indices = top_indices.cpu().numpy().tolist()\n",
    "    \n",
    "    retrieved_labels = [all_labels_cpu[idx].item() for idx in top_indices]\n",
    "    print(f\"Top indices: {top_indices}, Labels: {retrieved_labels}\")\n",
    "\n",
    "    result_images = [original_mnist_test[idx][0] for idx in top_indices]\n",
    "\n",
    "    # Generate Plot\n",
    "    text_features_cpu = text_features.cpu().numpy()\n",
    "    text_embedding_2d = reducer.transform(text_features_cpu)\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.scatterplot(\n",
    "        x=umap_embeddings_2d[:, 0], y=umap_embeddings_2d[:, 1],\n",
    "        hue=umap_labels_np, palette=sns.color_palette(\"tab10\", 10),\n",
    "        s=10, alpha=0.6, ax=ax\n",
    "    )\n",
    "    ax.scatter(\n",
    "        text_embedding_2d[0, 0], text_embedding_2d[0, 1],\n",
    "        marker='s', color='black', s=100, label=f'Text: \"{prompt}\"'\n",
    "    )\n",
    "    ax.set_title('UMAP Projection (Image Embeddings + Text Query)')\n",
    "    ax.set_xlabel('UMAP Component 1')\n",
    "    ax.set_ylabel('UMAP Component 2')\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    dot_marker_size_scale = 6\n",
    "    for i in range(min(10, len(handles) -1)):\n",
    "         if hasattr(handles[i], 'set_sizes'):\n",
    "             current_size = handles[i].get_sizes()[0]\n",
    "             handles[i].set_sizes([current_size * dot_marker_size_scale])\n",
    "    ax.legend(handles, labels, title='Digit Label / Text', loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    plt.close(fig)\n",
    "\n",
    "    return result_images, fig\n",
    "\n",
    "# --- Sketch Preprocessing ---\n",
    "def preprocess_sketch(sketch_input_data):\n",
    "    sketch_array = None\n",
    "    if sketch_input_data is None: return None\n",
    "    if isinstance(sketch_input_data, dict):\n",
    "        if 'image' in sketch_input_data and isinstance(sketch_input_data['image'], np.ndarray):\n",
    "             sketch_array = sketch_input_data['image']\n",
    "        elif 'composite' in sketch_input_data and isinstance(sketch_input_data['composite'], np.ndarray):\n",
    "             sketch_array = sketch_input_data['composite']\n",
    "        else: return None\n",
    "    elif isinstance(sketch_input_data, np.ndarray):\n",
    "        sketch_array = sketch_input_data\n",
    "    else: return None\n",
    "    if sketch_array is None or sketch_array.size == 0: return None\n",
    "\n",
    "    try:\n",
    "        pil_image = Image.fromarray(sketch_array).convert(\"L\")\n",
    "    except Exception: return None\n",
    "\n",
    "    pil_image = ImageOps.invert(pil_image)\n",
    "    bbox = pil_image.getbbox()\n",
    "    if bbox is None: return Image.new('L', (28, 28), color=0)\n",
    "    pil_image = pil_image.crop(bbox)\n",
    "    width, height = pil_image.size\n",
    "    target_size = max(width, height)\n",
    "    padding = ((target_size - width) // 2, (target_size - height) // 2,\n",
    "               target_size - width - (target_size - width) // 2,\n",
    "               target_size - height - (target_size - height) // 2)\n",
    "    pil_image = ImageOps.expand(pil_image, padding, fill=0)\n",
    "    pil_image = pil_image.resize((28, 28), Image.Resampling.LANCZOS)\n",
    "    return pil_image\n",
    "\n",
    "# --- Sketch Search ---\n",
    "def sketch_search_and_plot(sketch_image, top_k=3):\n",
    "    if sketch_image is None:\n",
    "        return [], None\n",
    "\n",
    "    print(\"Processing sketch...\")\n",
    "    preprocessed_sketch_pil = preprocess_sketch(sketch_image)\n",
    "    if preprocessed_sketch_pil is None:\n",
    "        print(\"Sketch preprocessing failed.\")\n",
    "        return [], None\n",
    "\n",
    "    clip_input_tensor = preprocess(preprocessed_sketch_pil).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        sketch_features = model.encode_image(clip_input_tensor)\n",
    "\n",
    "    print(\"Calculating similarities...\")\n",
    "    similarities = calculate_similarity(sketch_features, all_features_gpu)\n",
    "    _, top_indices = torch.topk(similarities.squeeze(0), top_k)\n",
    "    top_indices = top_indices.cpu().numpy().tolist()\n",
    "    print(f\"Top sketch indices: {top_indices}\")\n",
    "    \n",
    "    retrieved_labels = [all_labels_cpu[idx].item() for idx in top_indices]\n",
    "    print(f\"Labels for top indices: {retrieved_labels}\")\n",
    "\n",
    "    result_images = [original_mnist_test[idx][0] for idx in top_indices]\n",
    "\n",
    "    # Generate Plot\n",
    "    try:\n",
    "        sketch_features_cpu = sketch_features.cpu().numpy()\n",
    "        sketch_embedding_2d = reducer.transform(sketch_features_cpu)\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111)\n",
    "        sns.scatterplot(\n",
    "            x=umap_embeddings_2d[:, 0], y=umap_embeddings_2d[:, 1],\n",
    "            hue=umap_labels_np, palette=sns.color_palette(\"tab10\", 10),\n",
    "            s=10, alpha=0.6, ax=ax\n",
    "        )\n",
    "        ax.scatter(\n",
    "            sketch_embedding_2d[0, 0], sketch_embedding_2d[0, 1],\n",
    "            marker='s', color='black', s=100, label='Your Sketch'\n",
    "        )\n",
    "        ax.set_title('UMAP Projection (Image Embeddings + Sketch Query)')\n",
    "        ax.set_xlabel('UMAP Component 1')\n",
    "        ax.set_ylabel('UMAP Component 2')\n",
    "        ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        dot_marker_size_scale = 6\n",
    "        for i in range(min(10, len(handles) -1)):\n",
    "             if hasattr(handles[i], 'set_sizes'):\n",
    "                 current_size = handles[i].get_sizes()[0]\n",
    "                 handles[i].set_sizes([current_size * dot_marker_size_scale])\n",
    "        ax.legend(handles, labels, title='Digit Label / Sketch', loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "        plt.close(fig)\n",
    "        plot_output = fig\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating plot for sketch: {e}\")\n",
    "        plot_output = None\n",
    "\n",
    "    return result_images, plot_output\n",
    "\n",
    "# --- 7. Gradio Interface ---\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as app_interface:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # CLIP MNIST Search Engine üß†üñºÔ∏è‚úèÔ∏è\n",
    "        Use CLIP embeddings to search the MNIST dataset using text prompts or sketches.\n",
    "        **Model: Fine-Tuned on MNIST Training Set**\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Tabs():\n",
    "        # --- Text Search Tab ---\n",
    "        with gr.TabItem(\"Text Search\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    text_input = gr.Radio(\n",
    "                        choices=[str(i) for i in range(10)],\n",
    "                        label=\"Select a Digit\",\n",
    "                        info=\"Choose the digit you want to search for.\"\n",
    "                    )\n",
    "                    submit_btn_text = gr.Button(\"Search Text\", variant=\"primary\")\n",
    "                with gr.Column(scale=3):\n",
    "                    gallery_text = gr.Gallery(\n",
    "                        label=\"Search Results\", columns=3, object_fit=\"contain\",\n",
    "                        height=250, preview=True\n",
    "                    )\n",
    "                    plot_text = gr.Plot(label=\"UMAP Visualization (Text Query)\")\n",
    "\n",
    "        # --- Sketch Search Tab ---\n",
    "        with gr.TabItem(\"Sketch Search\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    sketch_input = gr.Sketchpad(\n",
    "                        label=\"Draw a Digit (0-9)\",\n",
    "                        type=\"numpy\",\n",
    "                     )\n",
    "                    submit_btn_sketch = gr.Button(\"Search Sketch\", variant=\"primary\")\n",
    "                    clear_btn_sketch = gr.Button(\"Clear Sketch\")\n",
    "                with gr.Column(scale=3):\n",
    "                     gallery_sketch = gr.Gallery(\n",
    "                        label=\"Search Results\", columns=3, object_fit=\"contain\",\n",
    "                        height=250, preview=True\n",
    "                    )\n",
    "                     plot_sketch = gr.Plot(label=\"UMAP Visualization (Sketch Query)\")\n",
    "\n",
    "    # --- Event Handlers ---\n",
    "    submit_btn_text.click(\n",
    "        fn=text_search_and_plot,\n",
    "        inputs=text_input,\n",
    "        outputs=[gallery_text, plot_text]\n",
    "    )\n",
    "    submit_btn_sketch.click(\n",
    "        fn=sketch_search_and_plot,\n",
    "        inputs=sketch_input,\n",
    "        outputs=[gallery_sketch, plot_sketch]\n",
    "    )\n",
    "    clear_btn_sketch.click(fn=clear_sketch, inputs=None, outputs=sketch_input)\n",
    "\n",
    "# --- Launch the app ---\n",
    "if __name__ == \"__main__\":\n",
    "    app_interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
